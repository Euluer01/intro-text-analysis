---
title: "Quantitative Text Analysis - Week 2"
output: html_notebook
bibliography: ../references.bib
---

# Meeting 2

## Resources

- rperseus: https://ropensci.github.io/rperseus/index.html

## Catch up and review

### Reading a file into memory

Can you read one of the files from last week into memory in RStudio? Enter the code to do so below.

```{r}
# your code goes here
```

### Visualizing data

#### Discuss 

- What kind(s) of visualization would be best for showing the relative frequencies of a verb like καλός in the Platonic corpus versus Thucydides?

Refer to @Brezina2018 [ch. 1] if you feel stuck.

## Installing R Packages

Generally speaking, you'll install packages in R using the `install.packages()` command. You can either run it in a cell in the interactive notebook or in the console below.

Let's try installing the "tidyverse" package, a collection of useful data science tools. Note that if you've already gone through the [review notebook](../r-review/00_r-review.Rmd), you can skip this step.

```{r}
install.packages("tidyverse")
```

This command only works for packages that are available on CRAN. For other packages, such as `"ropensci/rperseus"`, will have their own installation procedures, usually detailed in their README files.

## Downloading a corpus with PerseusR

The [rperseus](https://ropensci.github.io/rperseus/index.html) package (not to be confused with the PerseusR package on CRAN) lets us access texts from the Tufts Perseus Digital Library. As detailed in their README, the package can be installed as shown below.

Note that you might get an alert that says "Package devtools required but is not installed." You can click "Install" to install it.

```{r}
devtools::install_github("ropensci/rperseus")
```

With the package installed, you can peruse the `perseus_catalog`, which rperseus includes as a lazily loaded data frame.

::: {.callout}
What does a "lazily loaded data frame" mean? Explain it in your own words.
:::

In the rperseus README, there is an example for loading Virgil's _Aeneid_. How might we modify this example to load Pausanias?

```{r}
library(dplyr)
library(purrr)
library(rperseus)

# NB: You might see documentation that uses `%>%` instead
# of `|>`. They're functionally equivalent, but `|>` is now
# built into R and the preferred way of piping data. It's
# also easier to type (one less character, and keys that are
# generally easier to reach than <kbd>%</kbd>).
pausanias_grc <- perseus_catalog |>
  filter(group_name == "Pausanias", 
         language == "grc") |>
  pull(urn) |>
  get_perseus_text()
```

::: {.callout}
If you already know the URN of the text that you'd like to retrieve, how can you simplify the call to `get_perseus_text()` above?
:::

## Working with textual data

Let's take a look at `pausanias_grc`. In your console, run `View(pausanias_grc)`.

As you can see, `pausanias_grc` is a table (actually a ["tibble"](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html)) with 10 rows, corresponding to the 10 books in Pausanias' _Periegesis_, and 7 columns that contain metadata about the text.

::: {.callout}

Discuss: What units can we break Pausanias down into to make it more manageable? Don't worry about how you would do it in code yet, just think about how you might explore the units of the text.

:::

### Types of words

With all languages, but especially with heavily-inflected languages like ancient Greek and Latin, it is important to be precise about the kinds of word forms that we're dealing with.

#### Tokens 

A **token** or **running word** "is a single occurrence of a word form in the text" [@Brezina2018 39].

How can we count the number of tokens in Book 2 of Pausanias? The `rperseus` package lets us specify an excerpt from a larger text, so we can call

```{r}
pausanias_2 <- pausanias_grc[2,]
```

[`tidytext`](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) includes the function `unnest_tokens()` for "convert[ing] a dataframe with a text column to be one-token-per-row." So by definition, we can simply pass the result of `unnest_tokens(pausanias_2, word, text)` -- where `word` is the name of the column to output and `text` is the name of the column from the input dataframe-like object (such as a tibble) -- to `count()`. Like so:

```{r}
library(tidytext)

pausanias_2 |> unnest_tokens(word, text) |> count()
```

You should see that there are 22,961 **tokens** in Pausanias Book 2.

#### Types

A **type** is a unique word form in the corpus. For example, the inflected forms βουλεύεται and βουλεύομεν are each a type. (See @Brezina2018 [39-40].)

In other words, **types** are **tokens** grouped by form. So to count the number of **types** in Pausanias 2, we can do the following:

```{r}
pausanias_2_tokens <- pausanias_2 |> unnest_tokens(word, text)

# the `n_distinct` function 
n_distinct(pausanias_2_tokens$word)
```

What if we want to see the top `n` types in the corpus?

```{r}
pausanias_2_tokens |> 
  # group by the column `word`
  group_by(word) |> 
  count(sort = TRUE)
```

As you hopefully can see, these types aren't very interesting, and they're probably about the same for any text in the Greek corpus -- they don't tell us anything about Pausanias in particular. That's because we haven't filtered out **stop words**, words like δέ, καί, and the like that don't provide much information on their own. Luckily, `rperseus` comes with a list of stop words built in.

```{r}
pausanias_2_tokens |> 
  group_by(word) |> 
  count(sort = TRUE) |>
  anti_join(greek_stop_words)
```

That's better. We've filtered out over 100 types and have a clearer view, especially after the first 20 words or so (which still are mainly forms of εἰμί), of what types -- words in specific forms -- appear in Pausanias 2.

Be careful, though: these are still just raw counts, and they tell us very little about how we might characterize Pausanias 2 vis-à-vis a larger corpus.

#### Lemmata (or lemmas)

A **lemma** (plural **lemmata** or **lemmas**) represents "a group of all inflectional forms related to one stem that belong to the same word class (Kučera & Francis 1967: 1)" [@Brezina2018 40]. In simpler terms, a **lemma** is the dictionary form of a word, so **lemmata** give us a way of further reducing the word count. ἐστίν, ἔσμεν, and εἰσίν all have the same **lemma**: εἰμί.

Lemmatization, as you might guess, often involves additional processing. We could use `rperseus`'s `parse_excerpt` function, e.g., `parse_excerpt(pausanias_grc$urn[1], "2.1.1")`. But this is slow and runs out of memory on larger excerpts (like an entire book of the _Periegesis_).

Instead, we can use the `udpipe` package to annotate our text.

Install the package as usual: `install.packages("udpipe")`.

The pre-trained Greek pipeline is available in the directory above this one (if you've cloned the class repository), so we can load it like so:

```{r}
library(udpipe)

umodel_perseus <- udpipe_load_model("../ancient_greek-perseus-ud-2.5-191206.udpipe")
```

And we can run the annotation like so (refer to the [vignette](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html) for more info):

```{r}
annotated_pausanias_2 <- udpipe_annotate(umodel_perseus, x = pausanias_2$text, tagger = "default", parser = "none") |> as.data.frame()

# Examine a single row to check the results
annotated_pausanias_2[2,]
```

Just like we did with the types, we can group and count by lemmata:

```{r}
annotated_pausanias_2 |> 
  group_by(lemma) |> 
  count(sort = TRUE) |>
  anti_join(greek_stop_words, by = join_by(lemma == word))
```

And there we have it!

#### Lexemes

"A **lexeme** is a lemma with a particular meaning attached to it.... The best way of conceptualizing a lexeme is as a subentry in a dictionary" [@Brezina2018 40].

We won't deal with lexemes so much today, but it's important to remember that we can further group our lemmata according to specific dictionary meanings.

One challenge of working with lexemes is that, even with the advances of large language models like ChatGPT, there is no surefire way to annotate them automatically. We still need "human-in-the-loop" pipelines to catch errors and ambiguities. And keep in mind that even two humans might disagree on the lexeme for a particular word!

### Word-type wrap-ups

::: {.callout}
Discuss: How can we use these different notions of "word" in our analysis of corpora? Why is it important to be precise about what kind of word(s) we're using?
:::

## Homework

